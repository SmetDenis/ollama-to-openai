# Ollama to OpenAI Adapter Configuration Example
# Copy this file to config.yml and customize the settings

# Server configuration (required)
server:
  host: "127.0.0.1"    # IP address to bind to
  port: 11434          # Port to listen on (default Ollama port)

# OpenAI API configuration (required)
openai:
  # Your OpenAI API key (required)
  api_key: "sk-your-openai-api-key-here"

  # Custom OpenAI base URL (optional, defaults to https://api.openai.com/v1)
  # Useful for OpenAI-compatible APIs like Azure OpenAI, local LLMs, etc.
  # base_url: "https://your-custom-openai-endpoint.com/v1"

# Logging configuration (optional)
logging:
  log_level: "WARNING"    # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_requests: false     # Whether to log request/response details

# Model configurations (optional)
# If empty, all OpenAI models will be available with default settings
# If specified, only these models will be exposed through the adapter
# All parameters (except 'name') are passed directly to OpenAI API without validation
models:
  # Simple model name - no additional parameters
  - name: openai/gpt-4o-mini

  # Model with custom parameters - any OpenAI API parameters are supported
  - name: openai/gpt-4o
    temperature: 0.7
    max_tokens: 2000
    top_p: 0.9
    frequency_penalty: 0.1
    presence_penalty: 0.2

  # Model with response format (for structured outputs)
  - name: openai/gpt-4o-2024-08-06
    temperature: 0.1
    response_format:
      type: "json_object"

  # Model with advanced parameters
  - name: openai/gpt-3.5-turbo
    temperature: 0.5
    max_tokens: 1500
    stop: ["END", "STOP"]
    logit_bias:
      "50256": -100  # Suppress specific token
