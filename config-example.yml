# Ollama to OpenAI Adapter Configuration Example
# Copy this file to config.yml and customize the settings

# Server configuration (required)
server:
  host: "127.0.0.1"    # IP address to bind to
  port: 11434          # Port to listen on (default Ollama port)

# OpenAI API configuration (required)
openai:
  # Your OpenAI API key (required)
  api_key: "sk-your-openai-api-key-here"

  # Custom OpenAI base URL (optional, defaults to https://api.openai.com/v1)
  # Useful for OpenAI-compatible APIs like Azure OpenAI, local LLMs, etc.
  # base_url: "https://your-custom-openai-endpoint.com/v1"

  # List of allowed models (optional, if not specified all models will be available)
  # This filters which OpenAI models are exposed through the adapter
  allowed_models:
    - "openai/gpt-4o-mini"

# Logging configuration (optional)
logging:
  log_level: "WARNING"    # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_requests: false     # Whether to log request/response details

# Model-specific configurations (optional)
# Override default temperature for specific models
models:
  openai/gpt-4o-mini:
    temperature: 1
