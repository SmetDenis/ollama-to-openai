# Ollama to OpenAI Adapter - Manual Test Cases
# Use with REST Client extensions in VS Code or similar HTTP clients
# Make sure the server is running: python3 ollama_to_openai_adapter.py

### 1. Health Check - Verify service is running
GET http://localhost:11345/health
Accept: application/json

### 2. Root Endpoint - Service information
GET http://localhost:11345/
Accept: application/json

### 3. Version - Get adapter version
GET http://localhost:11345/api/version
Accept: application/json

### 4. List Models - GET all available models
GET http://localhost:11345/api/tags
Accept: application/json

### 5. List Models - POST format (search for specific model)
POST http://localhost:11345/api/tags
Accept: application/json
Content-Type: application/json

{
    "model": "gemini/gemini-2.5-flash"
}

### 6. Show Model Details - Get model information
POST http://localhost:11345/api/show
Accept: application/json
Content-Type: application/json

{
    "model": "gemini/gemini-2.5-flash"
}

### 7. Show Model Details - With :latest suffix
POST http://localhost:11345/api/show
Accept: application/json
Content-Type: application/json

{
    "model": "gemini/gemini-2.5-flash:latest"
}

### 8. Chat Completion - Simple non-streaming chat
POST http://localhost:11345/api/chat
Accept: application/json
Content-Type: application/json

{
    "model": "openai/gpt-5-mini",
    "messages": [
        {
            "role": "user",
            "content": "Ping?"
        }
    ],
    "stream": false
}

### 9. Chat Completion - Streaming chat
POST http://localhost:11345/api/chat
Accept: application/json
Content-Type: application/json

{
    "model": "openai/gpt-4o-mini",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "Write a short poem about coding."
        }
    ],
    "stream": true
}

### 10. Chat Completion - Multi-turn conversation
POST http://localhost:11345/api/chat
Accept: application/json
Content-Type: application/json

{
    "model": "openai/gpt-4o-mini",
    "messages": [
        {
            "role": "user",
            "content": "What is Python?"
        },
        {
            "role": "assistant",
            "content": "Python is a high-level programming language known for its simplicity and readability."
        },
        {
            "role": "user",
            "content": "What are its main advantages?"
        }
    ],
    "stream": false
}

### 11. Generate - Simple prompt completion (non-streaming)
POST http://localhost:11345/api/generate
Accept: application/json
Content-Type: application/json

{
    "model": "openai/gpt-4o-mini",
    "prompt": "Complete this sentence: The future of AI is",
    "stream": false
}

### 12. Generate - Streaming prompt completion
POST http://localhost:11345/api/generate
Accept: application/json
Content-Type: application/json

{
    "model": "openai/gpt-4o-mini",
    "prompt": "Write a brief explanation of machine learning:",
    "stream": true
}

### 15. List Running Models - Mock endpoint
GET http://localhost:11345/api/ps
Accept: application/json

### 16. Error Test - Invalid model name
POST http://localhost:11345/api/chat
Accept: application/json
Content-Type: application/json

{
    "model": "invalid-model-name",
    "messages": [
        {
            "role": "user",
            "content": "This should fail"
        }
    ]
}

### 17. Error Test - Missing required field
POST http://localhost:11345/api/chat
Accept: application/json
Content-Type: application/json

{
    "model": "gpt-4o-mini"
}

### 18. Error Test - Empty messages array
POST http://localhost:11345/api/chat
Accept: application/json
Content-Type: application/json

{
    "model": "gpt-4o-mini",
    "messages": []
}

### 19. Error Test - Invalid JSON
POST http://localhost:11345/api/chat
Accept: application/json
Content-Type: application/json

